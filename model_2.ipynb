{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:02:28.234022Z","iopub.status.busy":"2024-04-03T14:02:28.233279Z","iopub.status.idle":"2024-04-03T14:02:34.240729Z","shell.execute_reply":"2024-04-03T14:02:34.239977Z","shell.execute_reply.started":"2024-04-03T14:02:28.233982Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchtext.vocab as vocab\n","from torchtext.vocab import GloVe\n","import torch.nn as nn\n","from tqdm import tqdm\n","import numpy as np\n","import time\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from ordered_set import OrderedSet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:02:34.242569Z","iopub.status.busy":"2024-04-03T14:02:34.242156Z","iopub.status.idle":"2024-04-03T14:02:34.277790Z","shell.execute_reply":"2024-04-03T14:02:34.276557Z","shell.execute_reply.started":"2024-04-03T14:02:34.242544Z"},"trusted":true},"outputs":[],"source":["\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:02:34.280139Z","iopub.status.busy":"2024-04-03T14:02:34.279321Z","iopub.status.idle":"2024-04-03T14:02:34.298436Z","shell.execute_reply":"2024-04-03T14:02:34.297531Z","shell.execute_reply.started":"2024-04-03T14:02:34.280102Z"},"trusted":true},"outputs":[],"source":["special_tags=[\"<sos>\",\"<eos>\",\"<unk>\",\"<pad>\"]\n","symbols = [\"(\", \")\", \",\",\"|\",\"const_\"]\n","symbols.extend(range(10))\n","pos_special_tokens_prob=[0,0,0,0]\n","pos_special_tokens_sol=[0,0,0,0]\n","embed_dim=100\n","beam_size=10"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:02:34.304463Z","iopub.status.busy":"2024-04-03T14:02:34.304004Z","iopub.status.idle":"2024-04-03T14:02:34.313204Z","shell.execute_reply":"2024-04-03T14:02:34.312374Z","shell.execute_reply.started":"2024-04-03T14:02:34.304431Z"},"trusted":true},"outputs":[],"source":["def collate(batch):\n","    \n","    max_len_problem = max([len(sample[0]) for sample in batch])\n","    max_len_solution = max([len(sample[1]) for sample in batch])\n","    \n","    padded_prob = torch.empty((len(batch), max_len_problem), dtype=torch.long)\n","    padded_prob.fill_(pos_special_tokens_prob[3])\n","    padded_sol = torch.empty((len(batch), max_len_solution), dtype=torch.long)\n","    padded_sol.fill_(pos_special_tokens_sol[3])\n","    ans=torch.zeros(len(batch))\n","\n","    for idx in range(len(batch)):\n","        \n","        ans[idx]=batch[idx][2]\n","        padded_prob[idx, :len(batch[idx][0])] = torch.LongTensor(batch[idx][0])\n","        padded_sol[idx, :len(batch[idx][1])] = torch.LongTensor(batch[idx][1])\n","        \n","    return (padded_prob,padded_sol,ans)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:02:34.314681Z","iopub.status.busy":"2024-04-03T14:02:34.314414Z","iopub.status.idle":"2024-04-03T14:02:34.334888Z","shell.execute_reply":"2024-04-03T14:02:34.333972Z","shell.execute_reply.started":"2024-04-03T14:02:34.314659Z"},"trusted":true},"outputs":[],"source":["class load_data_train(Dataset):\n","    def __init__(self,json_path):\n","        self.path=json_path\n","        self.data=[]\n","        self.loaddata()\n","        self.problem_unique_words,self.sol_unique_words=self.gen_all_unique_words()\n","        self.problem_word2int = {word: i for i, word in enumerate(self.problem_unique_words)}\n","        self.problem_int2word = {i: word for word, i in self.problem_word2int.items()}\n","        self.sol_word2int = {word: i for i, word in enumerate(self.sol_unique_words)}\n","        self.sol_int2word = {i: word for word, i in self.sol_word2int.items()}\n","        self.max_problem_len=self.get_max_len()\n","        self.get_special_pos_prob()\n","        self.get_special_pos_sol()\n","    def get_special_pos_prob(self):\n","        for i,t in enumerate(special_tags):\n","            pos_special_tokens_prob[i]=self.problem_word2int[t]\n","            \n","    def get_special_pos_sol(self):\n","        for i,t in enumerate(special_tags):\n","            pos_special_tokens_sol[i]=self.sol_word2int[t]\n","            \n","    def gen_all_unique_words(self):\n","        u1=OrderedSet(special_tags)\n","        u2=OrderedSet(symbols+special_tags)\n","        for i,(prob,sol,a) in enumerate(self.data):\n","            for word in prob.split():\n","                u1.add(word)\n","            operations = sol.split(\"|\")\n","            l=self.tokanize_sol(i)\n","            for t in l:\n","                u2.add(t)\n","        return u1,u2\n","    \n","    def tokanize_problem(self,i):\n","        return self.data[i][0].split()\n","    \n","    def tokanize_sol(self,i):\n","        l=[]\n","        operations = self.data[i][1].split(\"|\")\n","        for j,operation in enumerate(operations):\n","            if not operation:  \n","                continue\n","            operation_name = operation.split(\"(\")[0]\n","            l.append(operation_name) \n","            l.append(\"(\")\n","            content = operation[operation.find(\"(\")+1:operation.find(\")\")]\n","            tokens = content.split(\",\")\n","            new=[]\n","            for i, token in enumerate(tokens):\n","                new.append(token)\n","                if i < len(tokens) - 1:\n","                    new.append(\",\")\n","            l.extend(new)\n","            l.append(\")\")\n","            if j < len(operations) - 1:\n","                    l.append(\"|\")\n","        return l\n","    \n","    def get_max_len(self):\n","        m=0\n","        for (p,s,a) in self.data:\n","            for p1 in p:\n","                m=max(m,len(p1.split()))\n","        return m\n","\n","    def loaddata(self):\n","        with open(self.path, 'r') as f:\n","            data = json.load(f)\n","            for i in data:\n","                #list(i[\"Problem\"].split())\n","#                 p=str(i[\"Problem\"]).split()\n","                self.data.append((i[\"Problem\"],i[\"linear_formula\"],i[\"answer\"]))\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, i):\n","        problem=[\"<sos>\"]+self.tokanize_problem(i)+[\"<eos>\"]\n","        sol=[\"<sos>\"]+self.tokanize_sol(i)+[\"<eos>\"]\n","        problem = [self.problem_word2int[q] if q in self.problem_word2int else self.problem_word2int[\"<unk>\"] for q in problem]\n","        sol = [self.sol_word2int[q] if q in self.sol_word2int else self.sol_word2int[\"<unk>\"] for q in sol]\n","        return ((problem,sol,self.data[i][2]))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:02:34.336211Z","iopub.status.busy":"2024-04-03T14:02:34.335926Z","iopub.status.idle":"2024-04-03T14:02:34.350816Z","shell.execute_reply":"2024-04-03T14:02:34.349908Z","shell.execute_reply.started":"2024-04-03T14:02:34.336189Z"},"trusted":true},"outputs":[],"source":["class load_data_test(Dataset):\n","    def __init__(self,json_path,train):\n","        self.path=json_path\n","        self.data=[]\n","        self.loaddata()\n","        self.problem_unique_words,self.sol_unique_words=train.problem_unique_words,train.sol_unique_words\n","        self.problem_word2int = train.problem_word2int\n","        self.problem_int2word = train.problem_int2word\n","        self.sol_word2int = train.sol_word2int\n","        self.sol_int2word =train.sol_int2word\n","    \n","    def tokanize_problem(self,i):\n","        return self.data[i][0].split()\n","    \n","    def tokanize_sol(self,i):\n","        l=[]\n","        operations = self.data[i][1].split(\"|\")\n","        for j,operation in enumerate(operations):\n","            if not operation:  \n","                continue\n","            operation_name = operation.split(\"(\")[0]\n","            l.append(operation_name) \n","            l.append(\"(\")\n","            content = operation[operation.find(\"(\")+1:operation.find(\")\")]\n","            tokens = content.split(\",\")\n","            new=[]\n","            for i, token in enumerate(tokens):\n","                new.append(token)\n","                if i < len(tokens) - 1:\n","                    new.append(\",\")\n","            l.extend(new)\n","            l.append(\")\")\n","            if j < len(operations) - 1:\n","                    l.append(\"|\")\n","        return l\n","\n","    def loaddata(self):\n","        with open(self.path, 'r') as f:\n","            data = json.load(f)\n","            for i in data:\n","                #list(i[\"Problem\"].split())\n","#                 p=str(i[\"Problem\"]).split()\n","                self.data.append((i[\"Problem\"],i[\"linear_formula\"],i[\"answer\"]))\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, i):\n","        problem=[\"<sos>\"]+self.tokanize_problem(i)+[\"<eos>\"]\n","        sol=[\"<sos>\"]+self.tokanize_sol(i)+[\"<eos>\"]\n","        problem = [self.problem_word2int[q] if q in self.problem_word2int else self.problem_word2int[\"<unk>\"] for q in problem]\n","        sol = [self.sol_word2int[q] if q in self.sol_word2int else self.sol_word2int[\"<unk>\"] for q in sol]\n","        return ((problem,sol,self.data[i][2]))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:05:02.643261Z","iopub.status.busy":"2024-04-03T14:05:02.642436Z","iopub.status.idle":"2024-04-03T14:05:04.883914Z","shell.execute_reply":"2024-04-03T14:05:04.883134Z","shell.execute_reply.started":"2024-04-03T14:05:02.643226Z"},"trusted":true},"outputs":[],"source":["train_path=\"/kaggle/input/new-wp/data/train.json\"\n","train_data=load_data_train(train_path)\n","train_loader = DataLoader(train_data, batch_size=64, shuffle=False, collate_fn=collate)\n","\n","test_path=\"/kaggle/input/new-wp/data/test.json\"\n","test_data=load_data_test(test_path,train_data)\n","test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate)\n","\n","\n","\n","validation_path=\"/kaggle/input/new-wp/data/dev.json\"\n","val_data=load_data_test(validation_path,train_data)\n","validation_loader = DataLoader(val_data, batch_size=64, shuffle=False, collate_fn=collate)\n","validation_loader1 = DataLoader(val_data, batch_size=1, shuffle=False, collate_fn=collate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:05:30.963103Z","iopub.status.busy":"2024-04-03T14:05:30.962743Z","iopub.status.idle":"2024-04-03T14:05:30.970671Z","shell.execute_reply":"2024-04-03T14:05:30.969679Z","shell.execute_reply.started":"2024-04-03T14:05:30.963075Z"},"trusted":true},"outputs":[],"source":["class GloveEmbeddings():\n","    def __init__(self, embed_dim, word2idx):\n","        self.embed_dim = embed_dim\n","        self.word2idx = word2idx\n","        self.vocab_size = len(word2idx)\n","    \n","    def get_embedding_matrix(self):\n","        glove = GloVe(name='6B', dim=self.embed_dim)\n","        embedding_matrix = torch.zeros((self.vocab_size, self.embed_dim)) #init zeros\n","        \n","        for i in pos_special_tokens_prob[:3]:\n","            embedding_matrix[i] = torch.randn(self.embed_dim)\n","            \n","        for k, v in self.word2idx.items():        \n","            if k in glove.stoi:\n","                embedding_matrix[v] = torch.tensor(glove.vectors[glove.stoi[k]])\n","            elif k not in special_tags:\n","                embedding_matrix[v] = embedding_matrix[pos_special_tokens_prob[2]]\n","#         embedding_matrix[]\n","        return embedding_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:05:31.217382Z","iopub.status.busy":"2024-04-03T14:05:31.217111Z","iopub.status.idle":"2024-04-03T14:05:31.221782Z","shell.execute_reply":"2024-04-03T14:05:31.220800Z","shell.execute_reply.started":"2024-04-03T14:05:31.217358Z"},"trusted":true},"outputs":[],"source":["glove=GloveEmbeddings(200,train_data.problem_word2int)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:05:33.155485Z","iopub.status.busy":"2024-04-03T14:05:33.154837Z","iopub.status.idle":"2024-04-03T14:09:16.000661Z","shell.execute_reply":"2024-04-03T14:09:15.999664Z","shell.execute_reply.started":"2024-04-03T14:05:33.155453Z"},"trusted":true},"outputs":[],"source":["embedding=glove.get_embedding_matrix()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:20:07.933002Z","iopub.status.busy":"2024-04-03T14:20:07.932273Z","iopub.status.idle":"2024-04-03T14:20:07.940745Z","shell.execute_reply":"2024-04-03T14:20:07.939642Z","shell.execute_reply.started":"2024-04-03T14:20:07.932967Z"},"trusted":true},"outputs":[],"source":["\n","class AttentionNetwork(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(AttentionNetwork, self).__init__()\n","        self.attention_fc = nn.Linear(hidden_dim * 3, hidden_dim)\n","        self.vector_projection = nn.Linear(hidden_dim, 1, bias=False)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, last_hidden_state, encoder_outputs):\n","        # Correctly repeating the hidden state to match the batch size of the encoder outputs\n","        repeated_hidden = last_hidden_state.repeat(encoder_outputs.size(1), 1, 1).transpose(0, 1)\n","        energy = torch.cat((repeated_hidden, encoder_outputs), dim=2)\n","        energy = torch.tanh(self.attention_fc(energy))\n","        attention_scores = self.vector_projection(energy).squeeze(2)\n","        attention_weights = self.softmax(attention_scores)\n","        context_vector = attention_weights.unsqueeze(1).bmm(encoder_outputs)\n","        return context_vector, attention_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:20:08.250671Z","iopub.status.busy":"2024-04-03T14:20:08.249917Z","iopub.status.idle":"2024-04-03T14:20:08.258811Z","shell.execute_reply":"2024-04-03T14:20:08.257908Z","shell.execute_reply.started":"2024-04-03T14:20:08.250645Z"},"trusted":true},"outputs":[],"source":["\n","    \n","          \n","class LSTMEncoder(nn.Module):\n","    def __init__(self, embedding_dim, hidden_units=256, embed_matrix=None):\n","        super(LSTMEncoder, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_units = hidden_units\n","        self.embedding = nn.Embedding.from_pretrained(embed_matrix, padding_idx=pos_special_tokens_prob[3])\n","        self.dropout = nn.Dropout(0.5) \n","        self.lstm = nn.LSTM(embedding_dim, hidden_units, num_layers=1, batch_first=True, \n","                            dropout=0.5, bidirectional=True)\n","        self.hidden_layer = nn.Linear(hidden_units * 2, hidden_units)  \n","        self.cell_layer = nn.Linear(hidden_units * 2, hidden_units)\n","\n","    def forward(self, inputs):\n","        embedded_inputs = self.dropout(self.embedding(inputs))  \n","        lstm_out, (hidden, cell) = self.lstm(embedded_inputs)\n","        hidden = self.hidden_layer(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n","        cell = self.cell_layer(torch.cat((cell[0:1], cell[1:2]), dim=2))\n","        return lstm_out, (hidden, cell)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:20:08.612311Z","iopub.status.busy":"2024-04-03T14:20:08.611592Z","iopub.status.idle":"2024-04-03T14:20:08.620071Z","shell.execute_reply":"2024-04-03T14:20:08.619081Z","shell.execute_reply.started":"2024-04-03T14:20:08.612282Z"},"trusted":true},"outputs":[],"source":[" \n","    \n","class LSTMAttnDecoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, units=256):\n","        super(LSTMAttnDecoder, self).__init__()\n","        self.embed_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=pos_special_tokens_sol[3])  \n","        self.dropout_layer = nn.Dropout(0.5)\n","        self.attn_layer = AttentionNetwork(units)\n","        self.lstm = nn.LSTM(embedding_dim + units * 2, units, num_layers = 1, bidirectional = False, dropout=0.5, batch_first=True)\n","        self.out = nn.Linear(units, vocab_size)\n","\n","    def forward(self, input_seq, hidden_state, encoder_outputs):\n","        embedded = self.dropout_layer(self.embed_layer(input_seq)).unsqueeze(1)\n","        attn_context, attn_weights = self.attn_layer(hidden_state[0], encoder_outputs)\n","        lstm_input = torch.cat([embedded, attn_context], dim=2)\n","        lstm_out, (h_n, c_n) = self.lstm(lstm_input, hidden_state)\n","        output = self.out(lstm_out)\n","        return output, (h_n, c_n)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:20:08.989197Z","iopub.status.busy":"2024-04-03T14:20:08.988640Z","iopub.status.idle":"2024-04-03T14:20:08.998654Z","shell.execute_reply":"2024-04-03T14:20:08.997711Z","shell.execute_reply.started":"2024-04-03T14:20:08.989171Z"},"trusted":true},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, embedding=embedding,embedding_dim=200,max_len_pred=None):\n","        super(Seq2Seq, self).__init__()\n","        self.lstm_encoder=LSTMEncoder(embedding_dim=embedding_dim,embed_matrix=embedding)\n","        self.lstm_decoder=LSTMAttnDecoder(vocab_size=max_len_pred,embedding_dim=embedding_dim)\n","        self.decoder_vocab_size=max_len_pred\n","    def forward(self, source_seq, target_seq, tf=0.9):\n","        src_batch_size = source_seq.size(0)\n","        tgt_seq_len = target_seq.size(1)\n","        \n","        encoder_states, (encoder_hidden, encoder_cell) = self.lstm_encoder(source_seq)\n","\n","        decoder_vocab_size = self.decoder_vocab_size\n","        decoder_outputs = torch.zeros(src_batch_size, tgt_seq_len, decoder_vocab_size)\n","        predicted_sequence = torch.zeros(src_batch_size, tgt_seq_len)\n","\n","        decoder_input_token = target_seq[:, 0]  # Initial decoder input\n","        predicted_sequence[:, 0] = decoder_input_token\n","        \n","        for step in range(1, tgt_seq_len):\n","            decoder_output, (encoder_hidden, encoder_cell) = self.lstm_decoder(decoder_input_token, (encoder_hidden, encoder_cell),encoder_states)\n","            decoder_output = decoder_output.squeeze(1)\n","            decoder_outputs[:, step, :] = decoder_output\n","            use_teacher_forcing = np.random.random() < tf\n","            decoder_input_token = target_seq[:, step] if use_teacher_forcing else decoder_output.argmax(dim=1)\n","            predicted_sequence[:, step] = decoder_output.argmax(dim=1)\n","\n","        return decoder_outputs, predicted_sequence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:20:09.616587Z","iopub.status.busy":"2024-04-03T14:20:09.615776Z","iopub.status.idle":"2024-04-03T14:20:09.659288Z","shell.execute_reply":"2024-04-03T14:20:09.658370Z","shell.execute_reply.started":"2024-04-03T14:20:09.616551Z"},"trusted":true},"outputs":[],"source":["model=Seq2Seq(embedding=embedding,embedding_dim=200,max_len_pred=len(train_data.sol_unique_words)).to(device)\n","def train(epochs):\n","    st=time.time()\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    train_l=[]\n","    val_l=[]\n","    for epoch in range(epochs):\n","        print(f\"=======================epoch {epoch}================================\")\n","        model.train()\n","        l=[]\n","        for j,data1 in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            x=data1[0].to(device)\n","            y=data1[1].to(device)\n","            o,w=model(x,y,tf=0.9)\n","            o = o.reshape(-1, o.shape[2]).to(device)\n","            y_orig = y.reshape(-1).to(device)\n","            loss = criterion(o, y_orig)\n","            loss.backward()\n","            optimizer.step()\n","            l.append(loss.item())\n","        train_l.append(np.mean(l))\n","        model.eval()\n","        l=[]\n","        for j,data1 in enumerate(validation_loader):\n","            x=data1[0].to(device)\n","            y=data1[1].to(device)\n","            o,w=model(x,y,tf=0)\n","            o = o.reshape(-1, o.shape[2]).to(device)\n","            y_orig = y.reshape(-1).to(device)\n","            loss = criterion(o, y_orig)\n","            l.append(loss.item())\n","        val_l.append(np.mean(l))\n","        print(f\"epoch : {epoch} train_loss : {train_l[-1]} val_loss: {val_l[-1]} time taken : {(time.time()-st)/60}  \")\n","        print(\"model saved!!\")\n","        torch.save(model, 'modelB_inloop.pth')\n","    return train_l,val_l"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:20:10.502236Z","iopub.status.busy":"2024-04-03T14:20:10.501681Z","iopub.status.idle":"2024-04-03T14:20:10.506070Z","shell.execute_reply":"2024-04-03T14:20:10.505173Z","shell.execute_reply.started":"2024-04-03T14:20:10.502203Z"},"trusted":true},"outputs":[],"source":["# import gc\n","# gc.collect()\n","\n","# # Clear memory allocated by PyTorch\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:20:10.937030Z","iopub.status.busy":"2024-04-03T14:20:10.936733Z","iopub.status.idle":"2024-04-03T14:27:16.855920Z","shell.execute_reply":"2024-04-03T14:27:16.854981Z","shell.execute_reply.started":"2024-04-03T14:20:10.937006Z"},"trusted":true},"outputs":[],"source":["train_l,val_l=train(50)\n","print(\"train completed!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-31T04:59:21.015526Z","iopub.status.busy":"2024-03-31T04:59:21.014624Z","iopub.status.idle":"2024-03-31T04:59:21.097815Z","shell.execute_reply":"2024-03-31T04:59:21.096881Z","shell.execute_reply.started":"2024-03-31T04:59:21.015499Z"},"trusted":true},"outputs":[],"source":["torch.save(model, 'modelB_final_50_epoch.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:29:14.590601Z","iopub.status.busy":"2024-04-03T14:29:14.590229Z","iopub.status.idle":"2024-04-03T14:29:14.599000Z","shell.execute_reply":"2024-04-03T14:29:14.598079Z","shell.execute_reply.started":"2024-04-03T14:29:14.590570Z"},"trusted":true},"outputs":[],"source":["def generate_csv(model,data,loader):\n","    model.eval()\n","    prediction=[]\n","    for batch in loader:\n","        x=batch[0].to(device)\n","        y=batch[1].to(device)\n","        o,w=model(x,y,tf=0)\n","        prediction.append(w)\n","    predicted_data_strings=[]\n","    for batch in prediction:\n","        for one_data in batch:\n","            one_data=one_data.to(torch.int)\n","            one_data_in_string=\"\"\n","            conv_int_word=[]\n","            for pos,value in enumerate(one_data):\n","                conv_int_word.append(data.sol_int2word[value.item()])\n","            predicted_data_strings.append(conv_int_word)\n","    return predicted_data_strings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:29:15.032278Z","iopub.status.busy":"2024-04-03T14:29:15.031454Z","iopub.status.idle":"2024-04-03T14:29:15.038695Z","shell.execute_reply":"2024-04-03T14:29:15.037750Z","shell.execute_reply.started":"2024-04-03T14:29:15.032245Z"},"trusted":true},"outputs":[],"source":["def extract_sol(data2str):\n","    complete_data=[]\n","    for each_data in data2str:\n","        s=\"\"\n","        flag=0\n","        for item in each_data[1:]:\n","            if(item==\"<eos>\" or item==\"<pad>\"):\n","                complete_data.append(s)\n","                flag=1\n","                break\n","            else:\n","                s+=str(item)\n","        if(flag==0):\n","            complete_data.append(s)\n","    return complete_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:29:15.702941Z","iopub.status.busy":"2024-04-03T14:29:15.702565Z","iopub.status.idle":"2024-04-03T14:29:15.709076Z","shell.execute_reply":"2024-04-03T14:29:15.708224Z","shell.execute_reply.started":"2024-04-03T14:29:15.702899Z"},"trusted":true},"outputs":[],"source":["def generate_sol(int2data,name,data_name):\n","    json_data=[]\n","    for k,s in zip(data_name.data,int2data):\n","        d={\n","            \"Problem\":k[0],\n","            \"answer\":k[2],\n","            \"predicted\":s,\n","            \"linear_formula\":k[1]\n","        }\n","        json_data.append(d)\n","    with open(str(name)+\".json\", 'w') as json_file:\n","        json.dump(json_data, json_file,indent=4)\n","    print(\"prediction json generated!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:29:17.322806Z","iopub.status.busy":"2024-04-03T14:29:17.321966Z","iopub.status.idle":"2024-04-03T14:29:53.561112Z","shell.execute_reply":"2024-04-03T14:29:53.560234Z","shell.execute_reply.started":"2024-04-03T14:29:17.322772Z"},"trusted":true},"outputs":[],"source":["data2string=generate_csv(model,train_data,train_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:29:53.562747Z","iopub.status.busy":"2024-04-03T14:29:53.562482Z","iopub.status.idle":"2024-04-03T14:29:53.569206Z","shell.execute_reply":"2024-04-03T14:29:53.568342Z","shell.execute_reply.started":"2024-04-03T14:29:53.562724Z"},"trusted":true},"outputs":[],"source":["len(data2string)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:34:34.003039Z","iopub.status.busy":"2024-04-03T14:34:34.002231Z","iopub.status.idle":"2024-04-03T14:34:34.165742Z","shell.execute_reply":"2024-04-03T14:34:34.165047Z","shell.execute_reply.started":"2024-04-03T14:34:34.003008Z"},"trusted":true},"outputs":[],"source":["good=extract_sol(data2string)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:34:35.404156Z","iopub.status.busy":"2024-04-03T14:34:35.403685Z","iopub.status.idle":"2024-04-03T14:34:35.410036Z","shell.execute_reply":"2024-04-03T14:34:35.408981Z","shell.execute_reply.started":"2024-04-03T14:34:35.404125Z"},"trusted":true},"outputs":[],"source":["len(good)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-03T14:34:47.544288Z","iopub.status.busy":"2024-04-03T14:34:47.543458Z","iopub.status.idle":"2024-04-03T14:34:47.833917Z","shell.execute_reply":"2024-04-03T14:34:47.832984Z","shell.execute_reply.started":"2024-04-03T14:34:47.544258Z"},"trusted":true},"outputs":[],"source":["generate_sol(good,\"train_data_model2\",train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import csv\n","def gen_csv(l,name):\n","    csv_file = name+\".csv\"\n","    with open(csv_file, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([\"Loss\"])  # Write header\n","        for loss in l:\n","            writer.writerow([loss])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen_csv(train_l,\"train_loss_model_2\")\n","gen_csv(val_l,\"val_loss_model_2\")\n","print(\"csv generated..\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4725486,"sourceId":8019679,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
